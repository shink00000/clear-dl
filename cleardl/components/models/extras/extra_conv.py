import torch.nn as nn


class ExtraConv(nn.Module):
    def __init__(self, feat_levels: list, in_channels: list, out_channels: int, max_level: int):
        """ Extra Feature Block (Generate by Convolution)

        Args:
            feat_levels (list): level of features generated by this module
            in_channels (list): number of feature channels output by the base classifier (up to max_level)
            out_channels (int): numver of extra feature channels
            max_level (int): max feature level of base network
        """
        super().__init__()

        self.feat_levels = feat_levels
        for i, level in enumerate(feat_levels):
            if level <= max_level:
                if in_channels[i] != out_channels:
                    m = nn.Conv2d(in_channels[i], out_channels, kernel_size=1)
                else:
                    m = nn.Identity()
            elif level == max_level + 1:
                m = nn.Conv2d(in_channels[i-1], out_channels, kernel_size=3, stride=2, padding=1)
            else:
                m = nn.Sequential(
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)
                )
            setattr(self, f'f{level}', m)

        self._init_weights()

    def forward(self, feats: dict):
        out_feats = {}
        for level in self.feat_levels:
            if level in feats:
                feat = feats[level]
            elif level-1 in feats:
                feat = feats[level-1]
            else:
                feat = out_feats[level-1]
            feat = getattr(self, f'f{level}')(feat)
            out_feats[level] = feat
        return out_feats

    def _init_weights(self):
        for name, m in self.named_modules():
            if name.startswith('aligner') and isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
